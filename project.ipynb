{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc86cab5-aa29-4765-a5ee-2dd5f606899a",
   "metadata": {},
   "source": [
    "Recall: real outliers we said are outliers / all points we said are outliers(right or wrong)\n",
    "Precision: real outliers we said are outliers / actual amount of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8313f-8f14-4ca7-a23e-6ebb613fe88e",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2eb6e084-f012-4b33-8414-2510d1a07b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import random\n",
    "\n",
    "def calculate_knn_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=5):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        \n",
    "        # Get the outlier scores for the sampled data\n",
    "        sample_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier score in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_Outlier_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_knn_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Convert outlier scores and ground truth labels to the same order\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Outlier_Score'].values\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile for demonstration\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c00ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.05433186490455213\n",
      "Recall: 0.27205882352941174\n",
      "AUC: 0.6857541591280174\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3740e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8524590163934426\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dbe13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0650887573964497\n",
      "Recall: 0.34375\n",
      "AUC: 0.7620468277945618\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2a65fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1594481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8434343434343434\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87a91aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9952718676122931\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d2de2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c7da918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0392156862745098\n",
      "Recall: 0.2222222222222222\n",
      "AUC: 0.666\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d628371-ff68-4e62-8ec5-6b61700684ad",
   "metadata": {},
   "source": [
    "LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c9cfd1ac-0993-4cb5-ada4-f6240daba008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_LOF_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        \n",
    "        # Get the outlier scores for the sampled data\n",
    "        sample_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier score in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_Outlier_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_LOF_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0299abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07929515418502203\n",
      "Recall: 0.39705882352941174\n",
      "AUC: 0.7569116543841843\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc896150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8538251366120219\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2a4d9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0650887573964497\n",
      "Recall: 0.34375\n",
      "AUC: 0.7878021148036254\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b065fbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6966666666666667\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d51b427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8181818181818181\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8939fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9917257683215129\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd3d67b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ff3bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6326666666666667\n"
     ]
    }
   ],
   "source": [
    "# LOF\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_LOF_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_LOF_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a4165",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5431306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_ABOD_outlier_scores(data, n_samples=5, sample_size=0.8):\n",
    "    \"\"\"\n",
    "    Calculates outlier scores using ABOD on random samples and averages scores\n",
    "    for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged ABOD scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        sample_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier scores in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_ABOD_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_ABOD_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_ABOD_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1cd5945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06607929515418502\n",
      "Recall: 0.33088235294117646\n",
      "AUC: 0.7693107100304488\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60afb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8060109289617485\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c266ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.05917159763313609\n",
      "Recall: 0.3125\n",
      "AUC: 0.715879909365559\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e683a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7433333333333334\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57a43b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.14285714285714285\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.7424242424242424\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cd306a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9763593380614658\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53d195f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5541c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0392156862745098\n",
      "Recall: 0.2222222222222222\n",
      "AUC: 0.6877777777777778\n"
     ]
    }
   ],
   "source": [
    "# ABOD\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_ABOD_outlier_scores(data, n_samples=15, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_ABOD_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4dd743-f710-4a7c-bd6d-ccf5f66ececb",
   "metadata": {},
   "source": [
    "KNN and LOF with avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4528ea27-625c-4413-907f-583d58bd531b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_knn_LOF_avg_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Normalize scores to [0, 1] range using MinMaxScaler\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        knn_scores_norm = minmax_scaler.fit_transform(knn_scores.reshape(-1, 1)).flatten()\n",
    "        lof_scores_norm = minmax_scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "        # Calculate the combined score as the average of KNN and LOF scores\n",
    "        combined_sample_scores = (knn_scores_norm + lof_scores_norm) / 2\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_knn_LOF_avg_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "26a86ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07342143906020558\n",
      "Recall: 0.36764705882352944\n",
      "AUC: 0.7323639733462778\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eb148457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8565573770491803\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0c6238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07692307692307693\n",
      "Recall: 0.40625\n",
      "AUC: 0.7818542296072507\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efbcd9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.75\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f7af60ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8181818181818181\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "91358414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9929078014184397\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "99e67606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eabcac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6497777777777778\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c4bcd-220b-4e8a-9597-b6a2709753ff",
   "metadata": {},
   "source": [
    "KNN and LOF with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbdfca9c-fbd6-4c34-bf9c-0289ea15984d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_knn_LOF_max_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Normalize scores to [0, 1] range using MinMaxScaler\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        knn_scores_norm = minmax_scaler.fit_transform(knn_scores.reshape(-1, 1)).flatten()\n",
    "        lof_scores_norm = minmax_scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Calculate the combined score as the max of KNN and LOF scores\n",
    "        combined_sample_scores = np.maximum(knn_scores_norm, lof_scores_norm)\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance across samples\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Max_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_knn_LOF_max_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Max_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c8587bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07048458149779736\n",
      "Recall: 0.35294117647058826\n",
      "AUC: 0.7282092581969021\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9054175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8524590163934426\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e909ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0650887573964497\n",
      "Recall: 0.34375\n",
      "AUC: 0.756797583081571\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores \n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a348bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "065e296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores \n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a0de6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9976359338061466\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "86cbca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d33b702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6557777777777778\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with max\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7216976-b57e-4933-ac8a-cafd0c03d694",
   "metadata": {},
   "source": [
    "KNN and LOF with min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b1bbcc1-6c4a-48e2-9f7e-4c259457e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_knn_LOF_min_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Normalize scores to [0, 1] range using MinMaxScaler\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        knn_scores_norm = minmax_scaler.fit_transform(knn_scores.reshape(-1, 1)).flatten()\n",
    "        lof_scores_norm = minmax_scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Calculate the combined score as the average of KNN and LOF scores\n",
    "        combined_sample_scores = np.minimum(knn_scores_norm , lof_scores_norm)\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_knn_LOF_min_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5e55dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07195301027900147\n",
      "Recall: 0.3602941176470588\n",
      "AUC: 0.7426459556065488\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "84be3e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8524590163934427\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with avg\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "26674e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07692307692307693\n",
      "Recall: 0.40625\n",
      "AUC: 0.7793429003021148\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "735f7731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7066666666666667\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fee20e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8282828282828282\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9e032fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9917257683215129\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "399d73e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec984c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6348888888888888\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF with min\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17488f3",
   "metadata": {},
   "source": [
    "KNN LOF ABOD with avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8e0a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Calculates combined outlier scores using LOF, ABOD, and KNN on random samples\n",
    "    and averages scores for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        n_neighbors (int): Number of neighbors for LOF and KNN algorithms.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged combined scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        abod_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Normalize scores to [0, 1] range using MinMaxScaler\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        knn_scores_norm = minmax_scaler.fit_transform(knn_scores.reshape(-1, 1)).flatten()\n",
    "        lof_scores_norm = minmax_scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "        abod_scores_norm = minmax_scaler.fit_transform(abod_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Calculate the combined score as the average of KNN, LOF, and ABOD scores\n",
    "        combined_sample_scores = (knn_scores_norm + lof_scores_norm + abod_scores_norm) / 3\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_knn_LOF_ABOD_avg_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ae6caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07195301027900147\n",
      "Recall: 0.3602941176470588\n",
      "AUC: 0.7318487710162835\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "80545bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8592896174863388\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores \n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "97b3dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07100591715976332\n",
      "Recall: 0.375\n",
      "AUC: 0.7831004531722054\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores \n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "abca3c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.75\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores \n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bb3a0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1923903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9952718676122931\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7c8ab998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "35f05cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6657777777777778\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with avg\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_avg_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_avg_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d62c0",
   "metadata": {},
   "source": [
    "KNN LOF ABOD with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6f58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Calculates combined outlier scores using the maximum score from LOF, ABOD, and KNN on random samples\n",
    "    and averages scores for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        n_neighbors (int): Number of neighbors for LOF and KNN algorithms.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged combined scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        abod_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        knn_scores_norm = minmax_scaler.fit_transform(knn_scores.reshape(-1, 1)).flatten()\n",
    "        lof_scores_norm = minmax_scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "        abod_scores_norm = minmax_scaler.fit_transform(abod_scores.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Calculate the combined score as the maximum of KNN, LOF, and ABOD scores\n",
    "        combined_sample_scores = np.maximum.reduce([knn_scores_norm, lof_scores_norm, abod_scores_norm])\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_knn_LOF_ABOD_max_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e060def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06461086637298091\n",
      "Recall: 0.3235294117647059\n",
      "AUC: 0.7663551917391114\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max \n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores \n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "be9dc860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.7978142076502732\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max\n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3cf652bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.05917159763313609\n",
      "Recall: 0.3125\n",
      "AUC: 0.7126321752265861\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max\n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ce6b5513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7433333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max\n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d320db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7373737373737373\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max\n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "956e8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9763593380614658\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max\n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "65e2a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max \n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4338ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0392156862745098\n",
      "Recall: 0.2222222222222222\n",
      "AUC: 0.6866666666666666\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with max\n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_max_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_max_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfce0a",
   "metadata": {},
   "source": [
    "KNN LOF ABOD with min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2cac3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Calculates combined outlier scores using the maximum score from LOF, ABOD, and KNN on random samples\n",
    "    and averages scores for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        n_neighbors (int): Number of neighbors for LOF and KNN algorithms.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged combined scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        abod_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Normalize scores to [0, 1] range using MinMaxScaler\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        knn_scores_norm = minmax_scaler.fit_transform(knn_scores.reshape(-1, 1)).flatten()\n",
    "        lof_scores_norm = minmax_scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "        abod_scores_norm = minmax_scaler.fit_transform(abod_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Calculate the combined score as the minimum of KNN, LOF, and ABOD scores\n",
    "        combined_sample_scores = np.minimum.reduce([knn_scores_norm, lof_scores_norm, abod_scores_norm])\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_knn_LOF_ABOD_min_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "58fe15e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07195301027900147\n",
      "Recall: 0.3602941176470588\n",
      "AUC: 0.7485437535854552\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Annthyroid_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ecfa7902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.04\n",
      "Recall: 0.3333333333333333\n",
      "AUC: 0.8579234972677596\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Arrhythmia_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ddf82fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0650887573964497\n",
      "Recall: 0.34375\n",
      "AUC: 0.7973942598187311\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Cardiotocography_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7b8f7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.7066666666666667\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: HeartDisease_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "128fef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2857142857142857\n",
      "Recall: 0.6666666666666666\n",
      "AUC: 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Hepatitis_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0af3c44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "AUC: 0.9917257683215129\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Lymphography_withoutdupl_norm_1ofn.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-2].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -2].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c0c6269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Parkinson_withoutdupl_norm_05_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3f1f0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "AUC: 0.6355555555555555\n"
     ]
    }
   ],
   "source": [
    "# KNN LOF ABOD with min \n",
    "# dataset: Pima_withoutdupl_norm_02_v01.csv\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_knn_LOF_ABOD_min_outlier_scores(data, n_samples=15, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_knn_LOF_ABOD_min_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585cb47b",
   "metadata": {},
   "source": [
    "dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d3a3ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully converted all datasets to csv file.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process_arff_to_csv(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Processes an ARFF file starting from @DATA section and converts it to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file_path (str): Path to the input .arff file.\n",
    "        output_file_path (str): Path to save the output .csv file.\n",
    "    \"\"\"\n",
    "    data_section = False  # Flag to start processing after @DATA\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w', newline='') as outfile:\n",
    "        csv_writer = csv.writer(outfile)\n",
    "        \n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check for @DATA to start processing\n",
    "            if line.upper() == \"@DATA\":\n",
    "                data_section = True\n",
    "                continue\n",
    "            \n",
    "            if data_section and line:  # Process only after @DATA and ignore empty lines\n",
    "                # Split the line by commas\n",
    "                values = line.split(\",\")\n",
    "                \n",
    "                # Convert last column: 'yes' -> 1, 'no' -> 0\n",
    "                if values[-1].strip() == \"'yes'\":\n",
    "                    values[-1] = 1\n",
    "                elif values[-1].strip() == \"'no'\":\n",
    "                    values[-1] = 0\n",
    "                \n",
    "                # Write the processed line to CSV\n",
    "                csv_writer.writerow(values)\n",
    "\n",
    "# File paths\n",
    "input_file1 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid\\\\Annthyroid_norm_02_v01.arff\"\n",
    "output_file1 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "\n",
    "# File paths\n",
    "input_file2 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.arff\"\n",
    "output_file2 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Arrhythmia\\\\Arrhythmia_withoutdupl_norm_02_v01.csv\"\n",
    "\n",
    "# File paths\n",
    "input_file3 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.arff\"\n",
    "output_file3 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Cardiotocography\\\\Cardiotocography_norm_02_v01.csv\"\n",
    "\n",
    "# File paths\n",
    "input_file4 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.arff\"\n",
    "output_file4 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\HeartDisease\\\\HeartDisease_withoutdupl_norm_02_v01.csv\"\n",
    "\n",
    "# File paths\n",
    "input_file5 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.arff\"\n",
    "output_file5 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Hepatitis\\\\Hepatitis_withoutdupl_norm_05_v01.csv\"\n",
    "\n",
    "# File paths\n",
    "input_file7 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.arff\"\n",
    "output_file7 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Parkinson\\\\Parkinson_withoutdupl_norm_05_v01.csv\"\n",
    "\n",
    "# File paths\n",
    "input_file8 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.arff\"\n",
    "output_file8 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Pima\\\\Pima_withoutdupl_norm_02_v01.csv\"\n",
    "\n",
    "# Convert ARFF to CSV\n",
    "process_arff_to_csv(input_file1, output_file1)\n",
    "\n",
    "process_arff_to_csv(input_file2, output_file2)\n",
    "\n",
    "process_arff_to_csv(input_file3, output_file3)\n",
    "\n",
    "process_arff_to_csv(input_file4, output_file4)\n",
    "\n",
    "process_arff_to_csv(input_file5, output_file5)\n",
    "\n",
    "process_arff_to_csv(input_file7, output_file7)\n",
    "\n",
    "process_arff_to_csv(input_file8, output_file8)\n",
    "\n",
    "print(\"successfully converted all datasets to csv file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1fed8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arff_to_csv_Lym(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Processes an ARFF file starting from @DATA section and converts it to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file_path (str): Path to the input .arff file.\n",
    "        output_file_path (str): Path to save the output .csv file.\n",
    "    \"\"\"\n",
    "    data_section = False  # Flag to start processing after @DATA\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w', newline='') as outfile:\n",
    "        csv_writer = csv.writer(outfile)\n",
    "        \n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check for @DATA to start processing\n",
    "            if line.upper() == \"@DATA\":\n",
    "                data_section = True\n",
    "                continue\n",
    "            \n",
    "            if data_section and line:  # Process only after @DATA and ignore empty lines\n",
    "                # Split the line by commas\n",
    "                values = line.split(\",\")\n",
    "                \n",
    "                # Convert last column: 'yes' -> 1, 'no' -> 0\n",
    "                if values[-2].strip() == \"'yes'\":\n",
    "                    values[-2] = 1\n",
    "                elif values[-2].strip() == \"'no'\":\n",
    "                    values[-2] = 0\n",
    "                \n",
    "                # Write the processed line to CSV\n",
    "                csv_writer.writerow(values)\n",
    "\n",
    "\n",
    "# File paths\n",
    "input_file6 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.arff\"\n",
    "output_file6 = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Lymphography\\\\Lymphography_withoutdupl_norm_1ofn.csv\"\n",
    "\n",
    "process_arff_to_csv_Lym(input_file6, output_file6)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b667cebad148e7b094a58ee81f940c685de1dd70a003a9ccdca4a5792431bee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
