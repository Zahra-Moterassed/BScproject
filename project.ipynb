{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc86cab5-aa29-4765-a5ee-2dd5f606899a",
   "metadata": {},
   "source": [
    "Recall: real outliers we said are outliers / all points we said are outliers(right or wrong)\n",
    "Precision: real outliers we said are outliers / actual amount of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8313f-8f14-4ca7-a23e-6ebb613fe88e",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb6e084-f012-4b33-8414-2510d1a07b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.17\n",
      "Recall: 0.16831683168316833\n",
      "AUC: 0.5079571360918071\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import random\n",
    "\n",
    "def calculate_outlier_scores_knn(data, n_samples=5, sample_size=0.8, n_neighbors=5):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        \n",
    "        # Get the outlier scores for the sampled data\n",
    "        sample_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier score in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_Outlier_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Convert outlier scores and ground truth labels to the same order\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Outlier_Score'].values\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile for demonstration\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 50)  # Random dataset with 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels (10% outliers)\n",
    "\n",
    "\n",
    "# Calculate outlier scores\n",
    "outlier_scores_df = calculate_outlier_scores_knn(data, n_samples=5, sample_size=0.8, n_neighbors=5)\n",
    "\n",
    "# Evaluate the outlier detection\n",
    "precision, recall, auc = evaluate_outlier_detection(outlier_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e29c0a",
   "metadata": {},
   "source": [
    "KNN \n",
    "dataset: Annthyroid_norm_02_v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d628371-ff68-4e62-8ec5-6b61700684ad",
   "metadata": {},
   "source": [
    "LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cfd1ac-0993-4cb5-ada4-f6240daba008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.09433962264150944\n",
      "AUC: 0.47587691528428516\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_outlier_scores_lof_pyod(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        \n",
    "        # Get the outlier scores for the sampled data\n",
    "        sample_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier score in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_Outlier_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 5)  # Generate a random dataset of 1000 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate outlier scores using LOF\n",
    "outlier_scores_df = calculate_outlier_scores_lof_pyod(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(outlier_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a4165",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5431306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.1111111111111111\n",
      "AUC: 0.6007326007326007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_abod_outlier_scores(data, n_samples=5, sample_size=0.8):\n",
    "    \"\"\"\n",
    "    Calculates outlier scores using ABOD on random samples and averages scores\n",
    "    for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged ABOD scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        sample_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier scores in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_ABOD_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_ABOD_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Generate a random dataset and ground truth labels\n",
    "data = np.random.rand(100, 5)  # Random dataset with 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=100, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate ABOD outlier scores\n",
    "abod_scores_df = calculate_abod_outlier_scores(data, n_samples=5, sample_size=0.8)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(abod_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4dd743-f710-4a7c-bd6d-ccf5f66ececb",
   "metadata": {},
   "source": [
    "KNN and LOF with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4528ea27-625c-4413-907f-583d58bd531b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.08\n",
      "Recall: 0.0851063829787234\n",
      "AUC: 0.5179770795171669\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Calculate the combined score as the average of KNN and LOF scores\n",
    "        combined_sample_scores = (knn_scores + lof_scores) / 2\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 50)  # Generate a random dataset of 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using both LOF and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c4bcd-220b-4e8a-9597-b6a2709753ff",
   "metadata": {},
   "source": [
    "KNN and LOF with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbdfca9c-fbd6-4c34-bf9c-0289ea15984d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 0.18181818181818182\n",
      "AUC: 0.5801838610827375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Calculate the combined score as the max of KNN and LOF scores\n",
    "        combined_sample_scores = np.maximum(knn_scores, lof_scores)\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance across samples\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Max_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Max_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(100, 5)  # Generate a random dataset of 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=100, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using the max of LOF and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7216976-b57e-4933-ac8a-cafd0c03d694",
   "metadata": {},
   "source": [
    "KNN and LOF with min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1bbcc1-6c4a-48e2-9f7e-4c259457e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.0970873786407767\n",
      "AUC: 0.4915807038834951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Calculate the combined score as the average of KNN and LOF scores\n",
    "        combined_sample_scores = np.minimum(knn_scores , lof_scores)\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 50)  # Generate a random dataset of 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using both LOF and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17488f3",
   "metadata": {},
   "source": [
    "KNN LOF ABOD with avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e0a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.14285714285714285\n",
      "AUC: 0.6113671274961597\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Calculates combined outlier scores using LOF, ABOD, and KNN on random samples\n",
    "    and averages scores for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        n_neighbors (int): Number of neighbors for LOF and KNN algorithms.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged combined scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        abod_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Calculate the combined score as the average of KNN, LOF, and ABOD scores\n",
    "        combined_sample_scores = (knn_scores + lof_scores + abod_scores) / 3\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Generate a random dataset and ground truth labels\n",
    "data = np.random.rand(100, 5)  # Random dataset with 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=100, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores\n",
    "combined_scores_df = calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d62c0",
   "metadata": {},
   "source": [
    "KNN LOF ABOD with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df6f58fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.06666666666666667\n",
      "AUC: 0.4752941176470588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Calculates combined outlier scores using the maximum score from LOF, ABOD, and KNN on random samples\n",
    "    and averages scores for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        n_neighbors (int): Number of neighbors for LOF and KNN algorithms.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged combined scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        abod_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Calculate the combined score as the maximum of KNN, LOF, and ABOD scores\n",
    "        combined_sample_scores = np.maximum.reduce([knn_scores, lof_scores, abod_scores])\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Generate a random dataset and ground truth labels\n",
    "data = np.random.rand(100, 5)  # Random dataset with 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=100, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using max of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfce0a",
   "metadata": {},
   "source": [
    "KNN LOF ABOD with min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cac3fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06764705882352941\n",
      "Recall: 0.3382352941176471\n",
      "AUC: 0.7694735378012486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Calculates combined outlier scores using the maximum score from LOF, ABOD, and KNN on random samples\n",
    "    and averages scores for data points appearing in multiple samples.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy array): Input dataset of shape (n_samples, n_features).\n",
    "        n_samples (int): Number of random samples to create.\n",
    "        sample_size (float): Proportion of the dataset to include in each sample.\n",
    "        n_neighbors (int): Number of neighbors for LOF and KNN algorithms.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the indices and averaged combined scores.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply ABOD from pyod\n",
    "        abod = ABOD()\n",
    "        abod.fit(sample_data)\n",
    "        abod_scores = abod.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Calculate the combined score as the minimum of KNN, LOF, and ABOD scores\n",
    "        combined_sample_scores = np.minimum.reduce([knn_scores, lof_scores, abod_scores])\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluates the outlier detection performance using precision, recall, and AUC.\n",
    "\n",
    "    Parameters:\n",
    "        outlier_scores_df (pd.DataFrame): DataFrame with outlier scores for each instance.\n",
    "        ground_truth (pd.Series or numpy array): Ground truth labels (1 = outlier, 0 = normal).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision, Recall, and AUC scores.\n",
    "    \"\"\"\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Generate a random dataset and ground truth labels\n",
    "#data = np.random.rand(100, 5)  # Random dataset with 100 samples and 5 features\n",
    "#labels = np.random.choice([0, 1], size=100, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "file_path = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate data (all columns except the last)\n",
    "data = df.iloc[:, :-1].values\n",
    "    \n",
    "    # Separate labels (last column)\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "# Calculate combined outlier scores using min of LOF, ABOD, and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b3585",
   "metadata": {},
   "source": [
    "dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a3ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed CSV saved to: G:\\Nazanin\\B project\\code\\dataset\\Annthyroid\\Annthyroid_norm_02_v01.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process_arff_to_csv(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Processes an ARFF file starting from @DATA section and converts it to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file_path (str): Path to the input .arff file.\n",
    "        output_file_path (str): Path to save the output .csv file.\n",
    "    \"\"\"\n",
    "    data_section = False  # Flag to start processing after @DATA\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w', newline='') as outfile:\n",
    "        csv_writer = csv.writer(outfile)\n",
    "        \n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Check for @DATA to start processing\n",
    "            if line.upper() == \"@DATA\":\n",
    "                data_section = True\n",
    "                continue\n",
    "            \n",
    "            if data_section and line:  # Process only after @DATA and ignore empty lines\n",
    "                # Split the line by commas\n",
    "                values = line.split(\",\")\n",
    "                \n",
    "                # Convert last column: 'yes' -> 1, 'no' -> 0\n",
    "                if values[-1].strip() == \"'yes'\":\n",
    "                    values[-1] = 1\n",
    "                elif values[-1].strip() == \"'no'\":\n",
    "                    values[-1] = 0\n",
    "                \n",
    "                # Write the processed line to CSV\n",
    "                csv_writer.writerow(values)\n",
    "\n",
    "# File paths\n",
    "input_file = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid\\\\Annthyroid_norm_02_v01.arff\"\n",
    "output_file = \"G:\\\\Nazanin\\\\B project\\\\code\\\\dataset\\\\Annthyroid\\\\Annthyroid_norm_02_v01.csv\"\n",
    "\n",
    "# Convert ARFF to CSV\n",
    "process_arff_to_csv(input_file, output_file)\n",
    "\n",
    "print(f\"Processed CSV saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b667cebad148e7b094a58ee81f940c685de1dd70a003a9ccdca4a5792431bee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
