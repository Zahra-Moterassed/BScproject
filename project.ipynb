{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc86cab5-aa29-4765-a5ee-2dd5f606899a",
   "metadata": {},
   "source": [
    "Recall: real outliers we said are outliers / all points we said are outliers(right or wrong)\n",
    "Precision: real outliers we said are outliers / actual amount of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8313f-8f14-4ca7-a23e-6ebb613fe88e",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2eb6e084-f012-4b33-8414-2510d1a07b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.09174311926605505\n",
      "AUC: 0.46731329605947347\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import random\n",
    "\n",
    "def calculate_outlier_scores_knn(data, n_samples=5, sample_size=0.8, n_neighbors=5):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        \n",
    "        # Get the outlier scores for the sampled data\n",
    "        sample_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier score in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_Outlier_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Convert outlier scores and ground truth labels to the same order\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Outlier_Score'].values\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile for demonstration\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 50)  # Random dataset with 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels (10% outliers)\n",
    "\n",
    "#statlog_shuttle = fetch_ucirepo(id=148) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "#data = statlog_shuttle.data.features \n",
    "#labels = statlog_shuttle.data.targets \n",
    "\n",
    "# Calculate outlier scores\n",
    "outlier_scores_df = calculate_outlier_scores_knn(data, n_samples=5, sample_size=0.8, n_neighbors=5)\n",
    "\n",
    "# Evaluate the outlier detection\n",
    "precision, recall, auc = evaluate_outlier_detection(outlier_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d628371-ff68-4e62-8ec5-6b61700684ad",
   "metadata": {},
   "source": [
    "LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9cfd1ac-0993-4cb5-ada4-f6240daba008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07\n",
      "Recall: 0.07446808510638298\n",
      "AUC: 0.4985146305950871\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_outlier_scores_lof_pyod(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    outlier_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        \n",
    "        # Get the outlier scores for the sampled data\n",
    "        sample_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Store the outlier score in the dictionary\n",
    "        for idx, score in zip(sample_indices, sample_scores):\n",
    "            outlier_scores[idx]['score'] += score\n",
    "            outlier_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average outlier score for each instance\n",
    "    avg_outlier_scores = {idx: scores['score'] / scores['count'] for idx, scores in outlier_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    outlier_scores_df = pd.DataFrame(list(avg_outlier_scores.items()), columns=['Index', 'Avg_Outlier_Score'])\n",
    "    \n",
    "    return outlier_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 5)  # Generate a random dataset of 1000 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate outlier scores using LOF\n",
    "outlier_scores_df = calculate_outlier_scores_lof_pyod(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(outlier_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77426b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_lof.py:283: UserWarning: n_neighbors (20) is greater than the total number of samples (7). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#X =  [[-1.1, 1, 5, 33, 4], [-1.5, 2, 4, 8, 3], [0.3, 111, 89, 46, 23], [0.5, 15, 11, 2, -3]]\n",
    "X =  [[-1.1, 1, 5, 33, 4], [-1.5, 2, 4, 8, 3],[-1.5, 2, 4, 8, 3], [-28, 2, 49, 7, 23], [18, 2, 4, 1, 13], [0.3, 111, 89, 46, 23], [0.5, 15, 11, 2, -3]]\n",
    "clf = LOF().fit(X)\n",
    "print(clf.predict([[0.1, 1, 5, 2, 1.5], [0.5, 2, 7, 1, 0.8], [90, 100, 45, 7, 31]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4dd743-f710-4a7c-bd6d-ccf5f66ececb",
   "metadata": {},
   "source": [
    "KNN and LOF with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4528ea27-625c-4413-907f-583d58bd531b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.11\n",
      "Recall: 0.12087912087912088\n",
      "AUC: 0.46427060288446426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Calculate the combined score as the average of KNN and LOF scores\n",
    "        combined_sample_scores = (knn_scores + lof_scores) / 2\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 50)  # Generate a random dataset of 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using both LOF and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c4bcd-220b-4e8a-9597-b6a2709753ff",
   "metadata": {},
   "source": [
    "KNN and LOF with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbdfca9c-fbd6-4c34-bf9c-0289ea15984d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2\n",
      "Recall: 0.18181818181818182\n",
      "AUC: 0.5801838610827375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Calculate the combined score as the max of KNN and LOF scores\n",
    "        combined_sample_scores = np.maximum(knn_scores, lof_scores)\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance across samples\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Max_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Max_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(100, 5)  # Generate a random dataset of 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=100, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using the max of LOF and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores_max(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7216976-b57e-4933-ac8a-cafd0c03d694",
   "metadata": {},
   "source": [
    "KNN and LOF with min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1bbcc1-6c4a-48e2-9f7e-4c259457e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1\n",
      "Recall: 0.0970873786407767\n",
      "AUC: 0.4915807038834951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_std = scaler.fit_transform(data)\n",
    "\n",
    "    # Dictionary to store cumulative outlier scores and counts for each instance\n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'count': 0})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a random sample of the data\n",
    "        sample_indices = random.sample(range(data.shape[0]), int(sample_size * data.shape[0]))\n",
    "        sample_data = data_std[sample_indices]\n",
    "        \n",
    "        # Apply KNN from pyod\n",
    "        knn = KNN(n_neighbors=n_neighbors, method='mean')\n",
    "        knn.fit(sample_data)\n",
    "        knn_scores = knn.decision_scores_  # higher score -> more outlier\n",
    "        \n",
    "        # Apply LOF from pyod\n",
    "        lof = LOF(n_neighbors=n_neighbors)\n",
    "        lof.fit(sample_data)\n",
    "        lof_scores = lof.decision_scores_  # higher score -> more outlier\n",
    "\n",
    "        # Calculate the combined score as the average of KNN and LOF scores\n",
    "        combined_sample_scores = np.minimum(knn_scores , lof_scores)\n",
    "\n",
    "        # Store the combined score in the dictionary for each data point in the sample\n",
    "        for idx, score in zip(sample_indices, combined_sample_scores):\n",
    "            combined_scores[idx]['score'] += score\n",
    "            combined_scores[idx]['count'] += 1\n",
    "\n",
    "    # Calculate the average combined score for each instance\n",
    "    avg_combined_scores = {idx: scores['score'] / scores['count'] for idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    combined_scores_df = pd.DataFrame(list(avg_combined_scores.items()), columns=['Index', 'Avg_Combined_Outlier_Score'])\n",
    "    \n",
    "    return combined_scores_df\n",
    "\n",
    "def evaluate_outlier_detection(outlier_scores_df, ground_truth):\n",
    "    # Sort outlier scores and ground truth labels by index for alignment\n",
    "    outlier_scores_df = outlier_scores_df.sort_values(by='Index').reset_index(drop=True)\n",
    "    ground_truth = ground_truth[outlier_scores_df['Index']].values\n",
    "\n",
    "    # Extract average outlier scores\n",
    "    outlier_scores = outlier_scores_df['Avg_Combined_Outlier_Score'].values\n",
    "\n",
    "    # Set a threshold to classify outliers and normal points\n",
    "    threshold = np.percentile(outlier_scores, 90)  # Setting threshold at the 90th percentile\n",
    "    predictions = (outlier_scores >= threshold).astype(int)  # Classify as 1 if above threshold (outlier), else 0\n",
    "\n",
    "    # Calculate precision, recall, and AUC\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "    auc = roc_auc_score(ground_truth, outlier_scores)\n",
    "\n",
    "    return precision, recall, auc\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your dataset and 'labels' contains ground truth labels where 1 = outlier and 0 = normal\n",
    "data = np.random.rand(1000, 50)  # Generate a random dataset of 100 samples and 5 features\n",
    "labels = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # Random labels with 10% outliers\n",
    "\n",
    "# Calculate combined outlier scores using both LOF and KNN\n",
    "combined_scores_df = calculate_combined_outlier_scores(data, n_samples=5, sample_size=0.8, n_neighbors=20)\n",
    "\n",
    "# Evaluate the outlier detection performance\n",
    "precision, recall, auc = evaluate_outlier_detection(combined_scores_df, pd.Series(labels))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c511dab6-5e17-4346-8d35-bfb514a1b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "covertype = fetch_ucirepo(id=31) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = covertype.data.features \n",
    "y = covertype.data.targets \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ad1e2-92d9-4552-8801-d058274c4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "statlog_shuttle = fetch_ucirepo(id=148) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_shuttle.data.features \n",
    "y = statlog_shuttle.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(statlog_shuttle.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(statlog_shuttle.variables) \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b667cebad148e7b094a58ee81f940c685de1dd70a003a9ccdca4a5792431bee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
