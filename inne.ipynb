{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Isolation-based anomaly detection using nearest-neighbor ensembles.\n",
    "Part of the codes are adapted from https://github.com/xhan97/inne\n",
    "\"\"\"\n",
    "# Author: Xin Han <xinhan197@gmail.com>\n",
    "# License: BSD 2 clause\n",
    "\n",
    "import numbers\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, check_random_state\n",
    "\n",
    "from pyod.models.base import BaseDetector\n",
    "from pyod.utils.utility import MAX_INT, invert_order\n",
    "\n",
    "MIN_FLOAT = np.finfo(float).eps  # Smallest possible float value to avoid division by zero\n",
    "\n",
    "class INNE(BaseDetector):\n",
    "    \"\"\" Isolation-based anomaly detection using nearest-neighbor ensembles.\n",
    "\n",
    "    The INNE algorithm uses the nearest neighbour ensemble to isolate anomalies.\n",
    "    It partitions the data space into regions using a subsample and determines \n",
    "    an isolation score for each region, allowing it to detect both global and \n",
    "    local anomalies.\n",
    "\n",
    "    See :cite:`bandaragoda2018isolation` for details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int, default=200\n",
    "        The number of base estimators in the ensemble.\n",
    "    max_samples : int or float, optional (default=\"auto\")\n",
    "        Number of samples for training each base estimator.\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        Proportion of outliers in the dataset, used for setting the anomaly threshold.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        Seed for random number generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=200, max_samples=\"auto\", contamination=0.1, random_state=None):\n",
    "        # Initialization of parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.contamination = contamination\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the anomaly detector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : Ignored\n",
    "            Not used in unsupervised methods.\n",
    "        \"\"\"\n",
    "        # Validate input X (and optionally y) for consistency\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        self._set_n_classes(y)  # Needed for compatibility with PyOD's BaseDetector\n",
    "\n",
    "        # Determine the number of samples\n",
    "        n_samples = X.shape[0]\n",
    "        if isinstance(self.max_samples, str):\n",
    "            if self.max_samples == \"auto\":\n",
    "                # Set max_samples to 8 or n_samples, whichever is smaller\n",
    "                max_samples = min(8, n_samples)\n",
    "            else:\n",
    "                raise ValueError(f\"max_samples ({self.max_samples}) is not supported.\")\n",
    "        elif isinstance(self.max_samples, numbers.Integral):\n",
    "            if self.max_samples > n_samples:\n",
    "                # Ensure max_samples is not greater than total samples\n",
    "                warn(f\"max_samples ({self.max_samples}) is greater than n_samples ({n_samples}). Setting max_samples to n_samples.\")\n",
    "                max_samples = n_samples\n",
    "            else:\n",
    "                max_samples = self.max_samples\n",
    "        else:\n",
    "            # max_samples should be a float in (0, 1] if it's not an int or \"auto\"\n",
    "            if not 0.0 < self.max_samples <= 1.0:\n",
    "                raise ValueError(f\"max_samples must be in (0, 1], got {self.max_samples}.\")\n",
    "            max_samples = int(self.max_samples * X.shape[0])\n",
    "\n",
    "        self.max_samples_ = max_samples  # Store the actual max_samples value\n",
    "\n",
    "        # Proceed to fit the model\n",
    "        self._fit(X)\n",
    "        self.decision_scores_ = invert_order(self._score_samples(X))  # Compute decision scores\n",
    "        self._process_decision_scores()  # Calculate threshold and labels\n",
    "        return self\n",
    "\n",
    "    def _fit(self, X):\n",
    "        \"\"\" Build nearest-neighbor ensembles based on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # this might change\n",
    "        self._centroids = np.empty([self.n_estimators, self.max_samples_, n_features])  # Stores centroids\n",
    "        self._ratio = np.empty([self.n_estimators, self.max_samples_])  # Stores ratio of distances\n",
    "        self._centroids_radius = np.empty([self.n_estimators, self.max_samples_])  # Stores radius of each hypersphere\n",
    "        # this might change\n",
    "\n",
    "        # Generate random seeds for reproducibility\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        self._seeds = random_state.randint(MAX_INT, size=self.n_estimators)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            rnd = check_random_state(self._seeds[i])\n",
    "            # Randomly select subsamples as centroids\n",
    "            center_index = rnd.choice(n_samples, self.max_samples_, replace=False)\n",
    "\n",
    "            #this might change\n",
    "            self._centroids[i] = X[center_index]\n",
    "            center_dist = euclidean_distances(self._centroids[i], self._centroids[i], squared=True)\n",
    "            np.fill_diagonal(center_dist, np.inf)  # Ignore self-distances (diagonal is set to infinity)\n",
    "            # Calculate the radius of each hypersphere (nearest neighbor distance)\n",
    "            self._centroids_radius[i] = np.amin(center_dist, axis=1)\n",
    "\n",
    "            # Find nearest neighbor indices and calculate distance ratios\n",
    "            cnn_index = np.argmin(center_dist, axis=1)\n",
    "            cnn_radius = self._centroids_radius[i][cnn_index]\n",
    "\n",
    "            # Calculate the ratio used for scoring\n",
    "            self._ratio[i] = 1 - (cnn_radius + MIN_FLOAT) / (self._centroids_radius[i] + MIN_FLOAT)\n",
    "\n",
    "            # this might change\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Predict anomaly scores for the input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])  # Ensure the model is fitted\n",
    "        # Return inverted outlier scores (larger values indicate anomalies)\n",
    "        return invert_order(self._score_samples(X))\n",
    "\n",
    "    def _score_samples(self, X):\n",
    "        \"\"\"Compute the anomaly score for each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=False)  # Validate input array\n",
    "        isolation_scores = np.ones([self.n_estimators, X.shape[0]])  # Initialize isolation scores\n",
    "\n",
    "        # Loop over each base estimator (ensemble member)\n",
    "        # this might change\n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate the distance between test points and centroids\n",
    "            x_dists = euclidean_distances(X, self._centroids[i], squared=True)\n",
    "            # Find samples covered by at least one hypersphere\n",
    "            cover_radius = np.where(x_dists <= self._centroids_radius[i], self._centroids_radius[i], np.nan)\n",
    "            x_covered = np.where(~np.isnan(cover_radius).all(axis=1))\n",
    "            # Identify the centroid with the smallest radius covering the sample\n",
    "            cnn_x = np.nanargmin(cover_radius[x_covered], axis=1)\n",
    "            isolation_scores[i][x_covered] = self._ratio[i][cnn_x]\n",
    "        #this might change\n",
    "\n",
    "        # Average the isolation scores across all estimators\n",
    "        scores = np.mean(isolation_scores, axis=0)\n",
    "        return -scores  # Return negative scores (lower is more abnormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "X =  [[-1.1], [0.3], [0.5], [100]]\n",
    "clf = INNE().fit(X)\n",
    "print(clf.predict([[0.1], [0.5], [90], [-1.5]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_lof.py:283: UserWarning: n_neighbors (20) is greater than the total number of samples (7). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lof import LOF\n",
    "X =  [[-1.1, 1, 5, 33, 4], [-1.5, 2, 4, 8, 3],[-1.5, 2, 4, 8, 3], [-28, 2, 49, 7, 23], [18, 2, 4, 1, 13], [0.3, 111, 89, 46, 23], [0.5, 15, 11, 2, -3]]\n",
    "clf = LOF().fit(X)\n",
    "print(clf.predict([[0.1, 1, 5, 2, 1.5], [0.5, 2, 7, 1, 0.8], [90, 100, 45, 7, 31]]))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b667cebad148e7b094a58ee81f940c685de1dd70a003a9ccdca4a5792431bee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
